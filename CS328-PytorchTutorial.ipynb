{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch\n",
    "\n",
    "(Adapted from [Deep Learning with PyTorch: A 60 Minute Blitz](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What is PyTorch?\n",
    "================\n",
    "\n",
    "It’s a Python based scientific computing package. It's mainly targeted at two sets of\n",
    "audiences:\n",
    "\n",
    "- A deep learning research platform that provides good flexibility\n",
    "   and speed. This is what we'll be using PyTorch for.\n",
    "- A replacement for NumPy to use the power of GPUs. If you want to know what a GPU is, <a href=\"https://en.wikipedia.org/wiki/Graphics_processing_unit\">you can check out the Wikipedia article after class</a>. We won't be using GPUs or learning about them in this class.\n",
    "\n",
    "Getting Started\n",
    "---------------\n",
    "\n",
    "### Tensors\n",
    "\n",
    "Tensors are similar to lists, except they make multidimensional computations easy. Work through the examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a 5x3 matrix, uninitialized:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill a tensor with zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.zero_()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a Tensor from python lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a randomly initialized matrix:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get tensor's size:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.size())\n",
    "print(x.shape) # Another way of getting the size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.Size`` is in fact a tuple, so it supports all tuple operations.</p></div>\n",
    "\n",
    "### Operations\n",
    "\n",
    "There are multiple syntaxes for operations. In the following\n",
    "example, we will take a look at the addition operation.\n",
    "\n",
    "Addition: syntax 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addition: syntax 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addition: providing an output tensor as argument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = torch.Tensor(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addition: in-place\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds x to y\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>Any operation that mutates a tensor in-place is post-fixed with an ``_``.\n",
    "    For example: ``x.copy_(y)``, ``x.t_()``, will change ``x``.</p></div>\n",
    "\n",
    "You can use two numbers within the indexing operation to refer to a specific spot, or use syntax like [:,2] to refer to a particular column (and [2,:] to refer to a particular row. Try playing with this in the cell below - make sure you can do the following:\n",
    "- Print out a specific row of the matrix using a single line of code. This one is done for you.\n",
    "- Print out a specific column of the matrix using a single line of code.\n",
    "- Replace a particular entry with another number\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)\n",
    "print(x[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resizing: If you want to resize/reshape tensor, you can use ``torch.view``. Use print statements in the cell below to understand what y and z are, and what is meant by each element of the returned size. Then make your own 4x6 matrix and reshape it into a 12x2 matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learn more later if you wish:**\n",
    "\n",
    "\n",
    "  100+ Tensor operations, including transposing, indexing, slicing,\n",
    "  mathematical operations, linear algebra, random numbers, etc.,\n",
    "  are described\n",
    "  [here](http://pytorch.org/docs/torch>).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Autograd: automatic differentiation\n",
    "===================================\n",
    "\n",
    "Central to all neural networks in PyTorch is the ``autograd`` package.\n",
    "Let’s first briefly visit this, and we will then go to training our\n",
    "first neural network.\n",
    "\n",
    "\n",
    "The ``autograd`` package provides automatic differentiation for all operations\n",
    "on Tensors. It is a define-by-run framework, which means that your backpropagation is\n",
    "defined by how your code is run, and that every single iteration can be\n",
    "different. (Not quite sure what that means? That's okay, you'll learn more below. If you want to follow up after class, <a href=\"https://towardsdatascience.com/battle-of-the-deep-learning-frameworks-part-i-cff0e3841750\">check out this blog post.</a>)\n",
    "\n",
    "Let us see this in more simple terms with some examples.\n",
    "\n",
    "\n",
    "Tensor\n",
    "--------\n",
    "``torch.Tensor`` is the central class of the package. If you set its attribute ``.requires_grad`` as ``True``, it starts to track all operations on it. When you finish your computation you can call ``.backward()`` and have all the gradients computed automatically (gradient is just a term for derivative when we have multiple variables - having the gradients computed automatically allows us to automatically perform backpropagation without figuring out the derivatives ourselves). The gradient for this tensor will be accumulated into .grad attribute.\n",
    "\n",
    "To stop a tensor from tracking history, you can call .detach() to detach it from the computation history, and to prevent future computation from being tracked.\n",
    "\n",
    "To prevent tracking history (and using memory), you can also wrap the code block in with torch.no_grad():. This can be particularly helpful when evaluating a model because the model may have trainable parameters with requires_grad=True, but for which we don’t need the gradients.\n",
    "\n",
    "There’s one more class which is very important for autograd implementation - a ``Function``.\n",
    "\n",
    "``Tensor`` and ``Function`` are interconnected and build up an acyclic graph, that encodes a complete history of computation. Each tensor has a ``.grad_fn`` attribute that references a ``Function`` that has created the ``Tensor`` (except for Tensors created by the user - their grad_fn is None).\n",
    "\n",
    "If you want to compute the derivatives, you can call ``.backward()`` on a ``Tensor``. If ``Tensor`` is a scalar (i.e. it holds a one element data), you don’t need to specify any arguments to ``backward()``, however if it has more elements, you need to specify a gradient argument that is a tensor of matching shape.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tensor and set requires_grad=True to track computation with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do an operation on the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``y`` was created as a result of an operation, so it has a ``grad_fn``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do more operations on y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients\n",
    "---------\n",
    "Let's perform backpropagation now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've performed backpropagation, we'll print the gradients (i.e. $d(out)/dx$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have got a matrix of ``4.5``. Let’s call the ``out``\n",
    "*Tensor* “$o$”.\n",
    "We have that $o = \\frac{1}{4}\\sum_i z_i$,\n",
    "$z_i = 3(x_i+2)^2$ and $z_i\\bigr\\rvert_{x_i=1} = 27$. Try computing the derivative here with respect to the variable $x_{i}$ (one of the elements of the original 2x2 matrix of x's). Do you end up with 4.5? Double click this cell to see the derivation, but try it with your partner first.\n",
    "\n",
    "<!--\n",
    "$\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)$, hence\n",
    "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$.\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do many amazing things with autograd! Work through what's happening in the example below, and why you get the gradient results that you do. (Ading some print statements may be helpful...) What happens if you change the input to backward? <a href=\"https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward\">You can look in the docs to learn a little more about backward if you like.</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(gradients)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learn more later if you wish:**\n",
    "\n",
    "Documentation of ``autograd`` and ``Function`` is at\n",
    "http://pytorch.org/docs/autograd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Neural Networks\n",
    "===============\n",
    "\n",
    "Neural networks can be constructed using the ``torch.nn`` package.\n",
    "\n",
    "Now that you had a glimpse of ``autograd``, ``nn`` depends on\n",
    "``autograd`` to define models and differentiate them.\n",
    "An ``nn.Module`` contains layers, and a method ``forward(input)`` that\n",
    "returns the ``output`` when given input.\n",
    "\n",
    "For example, look at this network that classifies digit images:\n",
    "\n",
    "![](images/mnist.png)\n",
    "\n",
    "It is a feed-forward network. It takes the input, feeds it\n",
    "through several layers one after the other, and then finally gives the\n",
    "output.\n",
    "\n",
    "A typical training procedure for a neural network is as follows:\n",
    "\n",
    "- Define the neural network that has some learnable parameters (or\n",
    "  weights)\n",
    "- Iterate over a dataset of inputs\n",
    "- Process input through the network\n",
    "- Compute the loss (how far is the output from being correct - the error that we talked about in class)\n",
    "- Propagate gradients back into the network’s parameters\n",
    "- Update the weights of the network, typically using a simple update rule:\n",
    "  ``weight = weight - learning_rate * gradient``\n",
    "\n",
    "This should sound familiar from our discussions in class.\n",
    "\n",
    "We'll define the network, and show how to process input and train it. If you want more detail on any of these points, <a href=\"https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#define-the-network\">the original tutorial gives more depth (mastery of this material is not needed for this class)</a>.\n",
    "\n",
    "Define the network\n",
    "------------------\n",
    "\n",
    "Let’s define a convolutional neural network that we'll use to recognize images of digits; read through the code below and try to get a sense of what's happening - note down what doesn't make sense, and return to those questions at the end of the lab:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.inputsize = 28 # Height of the input image (we assume square images)\n",
    "        kernel_size1 = 5 # How big the convolutions are\n",
    "        padding1 = (kernel_size1-1)//2 # We add extra zero columns to the sides of the image\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=kernel_size1, padding = padding1) # 10 filters (different convolution patterns) in first convolution layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        kernel_size2 = 5\n",
    "        padding2 = (kernel_size2-1)//2\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=kernel_size2, padding = padding2) # 20 filters in second conv. layer\n",
    "        # How many outputs we have for one dimension and 1 filter\n",
    "        # Based on formula here: https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d\n",
    "        output_size_one_dim= ((self.inputsize+2*padding1-(kernel_size1-1))/2 +2*padding2-(kernel_size2-1))/2 \n",
    "        self.fc1size=int(20*output_size_one_dim**2) # Total inputs to fully-connected layer is number of filters * total filter outputs in x direction * total filter outputs in y direction\n",
    "        self.fc1 = nn.Linear(self.fc1size, 50) # 50 hidden units in first fully-connected layer\n",
    "        self.fc2 = nn.Linear(50, 10) # 10 output units\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # first convolutional layer\n",
    "        h_conv1 = self.conv1(x)\n",
    "        h_conv1 = F.relu(h_conv1)# This is the rectilinear unit - a particular instance of the \"g\" function we talked about in class \n",
    "        h_conv1_pool = self.pool(h_conv1)\n",
    "\n",
    "        # second convolutional layer\n",
    "        h_conv2 = self.conv2(h_conv1_pool)\n",
    "        h_conv2 = F.relu(h_conv2) \n",
    "        h_conv2_pool = self.pool(h_conv2)\n",
    "\n",
    "        # fully-connected layer\n",
    "        h_fc1 = h_conv2_pool.view(-1, self.fc1size) # this reshapes the tensor, so it's flat to give as input to the fc layer    \n",
    "        h_fc1 = self.fc1(h_fc1)\n",
    "        h_fc1 = F.relu(h_fc1) \n",
    "\n",
    "\n",
    "        # classifier output\n",
    "        output = self.fc2(h_fc1)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only had to define the ``forward`` function, and the ``backward``\n",
    "function (where gradients are computed) is automatically defined for us\n",
    "using ``autograd``.\n",
    "You can use any of the Tensor operations in the ``forward`` function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a random 28x28 input:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(1, 1, 28, 28)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 10 outputs because we'll be learning to distinguish between 10 digits (0-9). The output unit with the highest activation will be our best guess at  the true digit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Recap:**\n",
    "\n",
    "  - ``torch.Tensor`` - A multi-dimensional array with support for autograd operations like backward(). Also holds the gradient w.r.t. the tensor.\n",
    "  -    nn.Module - Neural network module. \n",
    "  -    ``nn.Parameter`` - A kind of ``Tensor``, that is automatically registered as a parameter when assigned as an attribute to a Module.\n",
    "  -   autograd.Function - Implements *forward and backward definitions of an autograd operation*. Every Tensor operation creates at least a single Function node that connects to functions that created a Tensor and encodes its history.\n",
    "  \n",
    "  \n",
    "Let's load some data to make things more concrete and train our network.\n",
    "\n",
    "Loading MNIST data\n",
    "-------------\n",
    "For this tutorial, we will use the MNIST dataset of hand-drawn digits. MNIST is one of the most famous data sets in machine learning. It is a digit recognition task, where the goal is classify images of handwritten digits with right label, e.g, either '0','1','2', ... '9'. The training set consists of 60,000 images (6,000 per digit), and the test set of has 10,000 additional images. The images in MNIST are of\n",
    "size 28x28, i.e. greyscale images 28x28 pixels in size.\n",
    "\n",
    "``torchvision`` is a module that has data loaders for common datasets such as\n",
    "Imagenet, CIFAR10, MNIST, etc. and data transformers for images. <a href=\"https://pytorch.org/docs/stable/torchvision/datasets.html\">You can read about the available datasets here.</a> Run the code below to load the MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show some of the training images to see what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images[:4]))\n",
    "# print labels\n",
    "print(' '.join('%5s' % labels[j] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a loss function and optimizing\n",
    "-------------\n",
    "Loss is another word for the error we've been talking about in class. The loss function takes the (output, target) pair of inputs, and computes a\n",
    "value that estimates how far away the output is from the target. There are several different\n",
    "[loss functions](http://pytorch.org/docs/nn.html#loss-functions) under the\n",
    "nn package .\n",
    "A simple loss is: ``nn.MSELoss`` which computes the mean-squared error\n",
    "between the input and the target. This is similar to what we've been using in classe error, but averages over the outputs to give a single error value even though there are ten ouputs.\n",
    "\n",
    "For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = images[0] # The first image shown up above\n",
    "output = net(input.unsqueeze(0)) # Pytorch expects an extra dimension - as if we were passing in multiple images\n",
    "target = torch.randn(10)\n",
    "target.zero_()\n",
    "target[labels[0]] = 1 # Set the spot for the true label of the first image to 1\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "print(\"target:\",target)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(\"output:\",output)\n",
    "print(\"loss:\",loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a loss function, we'll improve the weights using (stochastic) gradient descent. \"Stochastic gradient descent\" is the form of gradient descent we've talked about most often in class: we optimize the weights after one (or a small batch) of training examples, rather than only after seeing the entire training set. This is computationally easier for large datasets, and in practice, updating with a small number of examples at once, rather than only one or the whole training set, tends to perform best.\n",
    "\n",
    "Here's an example of using gradient descent in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create the optimizer - SGD is stochastic gradient descent,\n",
    "# net is our neural net and lr is learning rate\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001) \n",
    "\n",
    "# to optimize for our single example (from above)\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input.unsqueeze(0))\n",
    "print(output)\n",
    "print(target.squeeze(0))\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update\n",
    "\n",
    "# Now we'll see how our loss changed\n",
    "output = net(input.unsqueeze(0))\n",
    "loss = criterion(output, target)\n",
    "print(loss) # Compare this to what you saw above for the loss - it's a little smaller; we'll need more data to do well though"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learn more later if you wish:**\n",
    "\n",
    "  The neural network package contains various modules and loss functions\n",
    "  that form the building blocks of deep neural networks. A full list with\n",
    "  documentation is [here](http://pytorch.org/docs/nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the network\n",
    "-------------\n",
    "\n",
    "To fully train the network, we'll do the same sort of thing as above, with the optimizer, except we'll run through the whole training set. Above, we defined train loader - here's the line again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100,\n",
    "                                          shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_size refers to how many images we'll use for each gradient descent update. So, this says to use 100 images for each update. The DataLoader class is set up so that when we iterate over the trainloader, the amount of data in each iteration is equal to the batch_size. ``shuffle`` means that each time we iterate through the trainloader again, the order of the images will be shuffled. (Confused by this? Try printing the ``shape`` of the labels or inputs in the \"Training loop\" cell below. On any tensor, the shape attribute is defined so you can say things like ``print(labels.shape)``. Because this is a big loop, I'd recommend saying ``break`` afterwards so you don't print thousands of lines...)\n",
    "\n",
    "Before we get to running the loop, though, we're going to make one other change. We're going to use a different loss function. Squared-error doesn't make that much sense for a classification problem with 10 classes because it will spend lots of time trying to exactly match the zeros on all the classes that are incorrect when what we care about is that the highest output is on the output node representing the correct class. We'll use the CrossEntropy loss function for this; <a href=\"https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss\">you can read about it in the pytorch documentation</a> if you'd like to know the details, but intuitively, this loss is treating the output nodes like probabilities of each class, and it decreases as the probability on the correct class increases. The cell right below this changes the criterion we'll optimize, and also sets up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=.01) # We don't actually need this line since we defined optimizer above, but just a reminder for where the gradient descent comes in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):  # Usually, we'd loop over the data multiple times, but to save time, we'll just do it once\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # We'd need the line below if we were using MSELoss because that criterion expects a tensor that has 10\n",
    "        # outputs. We don't need it if you use the cross entropy loss because that's specifically for classification\n",
    "        # problems and thus can work with getting just a label.\n",
    "        # labels = torch.eye(10).index_select(dim=0, index=labels)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what does that mean? The loss went down, but is it any good? We can check this by comparing the class label that the neural network\n",
    "outputs to the ground-truth (the label from the dataset. If the prediction is\n",
    "correct, we add the sample to the list of correct predictions.\n",
    "\n",
    "Let's try this with a few images from the test set. The test set is more digit data, but not digit data we've trained on (why might we want to focus on accuracy on a test set rather than accuracy on the training set?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "imshow(torchvision.utils.make_grid(images[:4]))\n",
    "print('GroundTruth: ', ' '.join('%5s' % labels[j] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let us see what the neural network thinks these examples above are:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(images)\n",
    "print(outputs[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs are activations for the 10 classes.\n",
    "The higher the activation for a class, the more the network\n",
    "thinks that the image is of the particular class.\n",
    "So, let's get the index with highest activation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % predicted[j]\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does it do? Compare the predicted to what you saw as the true labels above.\n",
    "\n",
    "Let's look at how the network performs on the whole dataset. We'll calculate how many of the 10000 test images it calculates correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "    break\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks waaay better than chance, which is 10% accuracy (randomly picking\n",
    "a class out of 10 classes).\n",
    "Seems like the network learned something, even if it's not perfect.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Extra time?\n",
    "---------\n",
    "If you have extra time in class while we're looking at the lab, there are lots of things you might try.\n",
    "- Go back to any questions you had after reading the neural net code. Which things do you still not understand? Try looking in the documentation to learn more - e.g. <a href=\"https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d\">the documentation for Conv2d</a> may help with understanding what's going on there.\n",
    "- Subsitute in a different dataset. <a href=\"https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.FashionMNIST\">FashionMNIST</a> is one dataset that has the same dimensions as MNIST. <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Read about it here.</a>.\n",
    "- Experiment with changing the learning rate. How does the loss change over time? You may want to add additional loops through the training set. Make sure you reinitialize the network each time (otherwise, it's already starting from pretty good weights!).\n",
    "- Calculate the accuracy on the training set and the test set at each iteration of training.\n",
    "- Examine some cases where the network makes errors - do they seem like reasonable errors to you?\n",
    "- Look in the documentation of PyTorch for the parameters of the optimizer. What other optimizers might you try? Do they make much of a difference? What happens if you use a different criterion, like MSELoss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
